# DeepSEA
Here we describe the steps necessary to run [train_model.py](train_model.py) as of Nov 2017.

## Current to-dos
- Incorporate plugins for logging information and monitoring.
- Re-evaluate whether or not the "test" mode would be useful in this framework.
- Try out a ResNet model within this framework.
- Data preparation is still not automated.
- Testing!!

## Data preparation

**TODO:** section about shell script and the files that need to be downloaded, dependencies, etc. before running the shell script -> automate as much of the data prep as you can.

### The mouse genome FASTA file
We are working with the [mm10_no_alt_analysis_set_ENCODE file](https://www.encodeproject.org/data-standards/reference-sequences/) provided by the ENCODE project.

#### Indexed FASTA file
The FASTA file was indexed by running `samtools faidx mm10_no_alt_analysis_set_ENCODE.fasta` on the command line after downloading [samtools-1.5](http://www.htslib.org/download/).

As a result, we end up with two files:
1. mm10_no_alt_analysis_set_ENCODE.fasta
2. mm10_no_alt_analysis_set_ENCODE.fasta.fai

where the indexed FASTA file (1)---**not** the .fai file---is an input to our script.

### The genomic features .bed file
[This query](https://www.encodeproject.org/search/?type=Experiment&assay_slims%21=Genotyping&assay_title=ChIP-seq&assay_title=DNase-seq&assay_title=ATAC-seq&replicates.library.biosample.donor.organism.scientific_name=Mus+musculus&files.file_type=bigBed+narrowPeak&files.file_type=bed+narrowPeak&files.file_type=bed+broadPeak&files.file_type=bigBed+broadPeak&files.file_type=bigBed+bed3%2B&files.file_type=bed+bed9&files.file_type=bigBed+bed9&status%21=archived) was used to retrieve ENCODE mouse ChIP-seq, DNase-seq, and ATAC-seq datasets.
In short, the query
- selects the organism _Mus musculus_
- ignores the "Genotyping" assay category
- selects the ChIP-seq, DNase-seq, and ATAC-seq assays
- selects bed and bigBed file types
- ignores experiments with an "Archived" status

Clicking the Download button brings up instructions for batch downloading.
By then downloading the `files.txt` file generated by this query (this file is already provided in the [ENCODE_mouse_data](ENCODE_mouse_data) directory) and running
```bash
cd ENCODE_mouse_data
xargs -n 1 curl -O -L < ./files.txt
```
we were able to download all of the datasets.

This data serves as the positive examples we will use as input to the sequence-level model.
We concatenate every file into a single tab-separated .bed file (we will refer to this as an aggregate .bed file) that contains the following columns, in order.
Note that labels are not provided in the .bed file:
- chromosome (chr)
- start (0-based)
- end
- feature (use information in `metadata.tsv`, explained below)
- metadata_index (for data provenance--the row number in `metadata.tsv` that corresponds to the file from which a row originated)

#### Assigning features
`metadata.tsv` is the first file downloaded from `files.txt`.
The metadata file specifies information about each file listed in `files.txt`.
Every row in one such file will be assigned the same feature in the resulting aggregate .bed file.
The feature is based on the metadata available for that file.
- ChIP-seq: "ChIP-seq", metadata columns "Experiment target" and "Biosample term name" for that row.
- DNase-seq: "DNase-seq", metadata column "Biosample term name" for that row.
- ATAC-seq: "ATAC-seq", metadata column "Biosample term name" for that row.

The information listed for each assay is space-separated and used as the feature.

#### Tabix-indexed file
The aggregate .bed file was compressed and tabix-indexed after downloading [htslib-1.5](http://www.htslib.org/download/).

The steps taken (on the command line):
- `sort -k1V -k2n -k3n unsorted_aggregate.bed > sorted_aggregate.bed`
    - This sorts the .bed file using the chr, start, end columns.
    - You can check this using `head -n20 sorted_aggregate.bed | column -t`
- `bgzip -c sorted_aggregate.bed > sorted_aggregate.bed.gz`
    - Compresses the file
- `tabix -p bed sorted_aggregate.bed.gz`

_Steps adapted from [slowkow/pytabix](https://github.com/slowkow/pytabix#how-to-prepare-a-file-for-tabix)._

As a result, we end up with three files:
1. sorted_aggregate.bed
2. sorted_aggregate.bed.gz
3. sorted_aggregate.bed.gz.tbi

where the files (1) and (2) are inputs to our script.

### Coordinates file
We also generate a file that only contains the genomic coordinates of each
entry in the tabix-indexed file. This reduces the size of the uncompressed
dataset from which we sample before querying the tabix-indexed file.

```bash
cut -f 1-3 sorted_aggregate.bed > coords_only.txt
```

### Distinct features file
A file of the distinct features found in the dataset is created by running

```bash
cut -f 5 sorted_aggregate.bed | sort -u > distinct_features.txt
```

## Training and testing the model

### Setup
We are using Python 3.6 via [anaconda3](https://www.anaconda.com/download/).
This README also assumes that you have access to CUDA.
Specifically, we use the `cudnn/cuda-8.0/6.0` module on Princeton's **tigergpu** cluster.

The [env-spec.txt](env-spec.txt) file provided in this repository can be used to build an identical conda environment on Linux x86-64 by running the following:
```bash
conda create --name myenv --file ./env-spec.txt
```
(Replace `myenv` with your desired environment name.)

If you'd like to install these packages manually, these are the appropriate commands:

- `conda create --name myenv`
- `conda install pytorch torchvision cuda80 -c soumith`
- `conda install -c bioconda pytabix`
- `conda install -c anaconda docopt`
- `conda install -c anaconda pandas`

**Note**: The `pyfaidx` package must be installed **after** the environment has been created by running the following commands:
```bash
source activate myenv
pip install pyfaidx
```
The `pyfaidx` package provided on a conda channel has not been updated and will not install properly.

**Please do this regardless of whether or not you have used the spec file to build your environment.**
